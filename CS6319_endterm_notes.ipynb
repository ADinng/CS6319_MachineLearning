{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vbGTyhDbQm_y",
        "n7J0kXJmXEt2"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOR5xKHGWspQgC8WLtmO7hP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADinng/CS6319_MachineLearning/blob/main/CS6319_endterm_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exam 24"
      ],
      "metadata": {
        "id": "7pvacuSDQ1jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exam 24-1题- clusterk-means and em，VarianceThreshold，chi2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# a. read the file (no header)\n",
        "data = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/main/Data.csv?raw=true\",header=None)\n",
        "\n",
        "# b. extract the categorical feture to use as cluster lables\n",
        "features = data.drop(0, axis=1)\n",
        "labs = data[0].copy()\n",
        "\n",
        "# c. apply imputation and rescaling if necessary\n",
        "    # 1) check for miss value\n",
        "features.describe()\n",
        "features.isnull().sum()\n",
        "\n",
        "    # 2)hand miss value\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "imputer.fit(features)\n",
        "X = imputer.transform(features)\n",
        "features = pd.DataFrame(X, columns=features.columns)\n",
        "\n",
        "    # 3)再次检查是否imputer成功\n",
        "features.isnull().sum()\n",
        "\n",
        "    # 4) rescaling -->use standscaler( minmax范围0-1)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "  # 1)check 有几个cluster(2选1)\n",
        "labs.value_counts()\n",
        "labs.unique()\n",
        "\n",
        "\n",
        "  # 2)apply kmeans and rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "kms= KMeans(n_clusters=3)\n",
        "kms.fit(features)\n",
        "print(kms.labels_)\n",
        "\n",
        "from sklearn import metrics\n",
        "kms_score = metrics.rand_score(labs, kms.labels_)\n",
        "print(\"KMeans Rand Score:\", kms_score)\n",
        "\n",
        "\n",
        "  # 3)apply EM\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components=3)\n",
        "gm.fit(features)\n",
        "prediction = gm.predict(features)\n",
        "print(prediction)\n",
        "\n",
        "gm_score = metrics.rand_score(labs, prediction)\n",
        "print(\"EM Rand Score:\", gm_score)\n",
        "\n",
        "\n",
        "# e) Apply the VarianceThreshold method to remove at least one feature. (4 marks)\n",
        "\n",
        "  # 1)检查每个特征对应的方差值是多少(其实就是每个列对应的方差值)(8列，排除最小的0.001)\n",
        "print(pd.DataFrame(features).var())\n",
        "\n",
        "  # 2)apply the variancethreshold method\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "features_new = selector.fit_transform(features)\n",
        "\n",
        "  # 3) 查看剩余的方差值\n",
        "print(pd.DataFrame(features_new).var())\n",
        "\n",
        "print(\"Original number of features:\", features.shape[1])\n",
        "print(\"Number of features after variance threshold filtering:\", features_new.shape[1])\n",
        "\n",
        "# f) Apply the Chi2 method to remove at least one more feature. (4 marks)\n",
        "\n",
        "  # 1)查看剩余特征个数-7个\n",
        "features_new.shape[1]\n",
        "\n",
        "  # 2) apply the chi2 method（上面查看剩余7个，这里写6,提取前6个）\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "selector = SelectKBest(chi2, k=6)\n",
        "features_new2 = selector.fit_transform(features_new,labs)\n",
        "\n",
        "# 3）检查过滤结果-发现提取了前6个\n",
        "features_new2.shape[1]\n",
        "\n",
        "\n",
        "print(\"Original number of features:\", features.shape[1])\n",
        "print(\"After VarianceThreshold:\", features_new.shape[1])\n",
        "print(\"After Chi2 selection:\", features_new2.shape[1])\n",
        "\n",
        "\n",
        "# d.Apply k-means and expectation maximisation.Use the rand score function and the cluster labels to compute clustering accuracy.\n",
        "\n",
        "if kms_score > gm_score:\n",
        "    print(\"KMeans is better, reapplying it\")\n",
        "else:\n",
        "    print(\"EM is better, reapplying it\")\n",
        "\n",
        "\n",
        "kms= KMeans(n_clusters=3)\n",
        "kms.fit(features_new2)\n",
        "\n",
        "kms_score = metrics.rand_score(labs, kms.labels_)\n",
        "print(kms_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "Ou11VnqPQ0iM",
        "outputId": "317bd88f-a32d-4c45-dc9b-733c516e495f",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "5    0\n",
              "6    0\n",
              "7    0\n",
              "8    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exam 2024-2题目 APriori\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "\n",
        "data = pd.read_csv(\"http://fimi.uantwerpen.be/data/mushroom.dat\",header=None,delim_whitespace=True)\n",
        "data\n",
        "\n",
        "records = []\n",
        "for i in range(0, 8124):\n",
        "  records.append([str(data.values[i,j]) for j in range(0,23)])\n",
        "print(len(records))\n",
        "\n",
        "# length exactly =2\n",
        "rules = apriori(records, min_support=0.02, min_confidence=0.5, min_lift=40, min_length=2, max_length=2)\n",
        "results = list(rules)\n",
        "print(len(results))\n",
        "\n",
        "for item in results:\n",
        "  pair = item[0]\n",
        "  items = [x for x in pair]\n",
        "  print(\"Rule: \"+items[0]+ \" -> \" + items[1])\n",
        "  print(\"Support: \" + str(item[1]))\n",
        "  print(\"Confidence: \"+ str(item[2][0][2]))\n",
        "  print(\"Lift: \"+ str(item[2][0][3]))\n",
        "  print(\"======================================\")"
      ],
      "metadata": {
        "id": "WPaEwnws-wwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exam 23"
      ],
      "metadata": {
        "id": "vbGTyhDbQm_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exam 23 - 1题, PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# a.read data\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\")\n",
        "\n",
        "# b.Extract the 12 numerical attributes. Do not impute or rescale them.\n",
        "data = data.iloc[:,:12]\n",
        "\n",
        "# c. Apply Principal Component Analysis (PCA) to find the two main components of the 12numerical attributes.\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pcadata = pca.fit_transform(data)\n",
        "\n",
        "# d.plot the two components using a scatter plot.\n",
        "import matplotlib.pyplot as plt\n",
        "pcadf = pd.DataFrame(data=pcadata, columns=['pc1','pc2'])\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(pcadf['pc1'],pcadf['pc2'],alpha=0.5)\n",
        "plt.title('PCA Results')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# find the total explained variance ratio of the two components\n",
        "# print(pca.explained_variance_ratio_)\n",
        "print(pca.explained_variance_.sum())"
      ],
      "metadata": {
        "id": "MhJBtBmHQ7bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exam 23 - 2题 Gaussian mixture model\n",
        "from numpy import hstack\n",
        "from numpy.random import normal\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# a. Generate artificial 1-dimensional data, containing 100 real numbers with mean 10 and standarddeviation 3, and 100 with mean 20 and standard deviation 2\n",
        "X1 = normal(loc=10, scale=3, size=100)\n",
        "X2 = normal(loc=20, scale=2, size=100)\n",
        "X = hstack((X1, X2))\n",
        "X = X.reshape((len(X),1))\n",
        "\n",
        "pyplot.hist(X, bins=30)\n",
        "pyplot.show()\n",
        "\n",
        "# b.Fit a Gaussian mixture model to the data. (因为两个正态分布x1,x2，所以n_components=2)\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components=2)\n",
        "gm.fit(X)\n",
        "\n",
        "# c.Use “predict” method to obtain a list of predicted cluster labels for the 200 data points,就是X全部有200个\n",
        "y = gm.predict(X)\n",
        "\n",
        "\n",
        "# d.Calculate how accurate the predictions are by measuring the rand score.\n",
        "# 需要计算实际标签，其实由0,1组成(写为100个0，100个1)\n",
        "true_labels = [0] * 100 + [1] * 100\n",
        "from sklearn import metrics\n",
        "gm_score = metrics.rand_score(true_labels, y)\n",
        "print(gm_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JBYktTSj7IM",
        "outputId": "fe0ec90e-2d2b-4a5f-ffab-162d7a2041e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9703015075376884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exam23 - 3题.apyori\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "\n",
        "# a. read data, and check data -->no header, separators use spaces\n",
        "data = pd.read_csv(\"http://fimi.uantwerpen.be/data/chess.dat\")\n",
        "data = pd.read_csv(\"http://fimi.uantwerpen.be/data/chess.dat\",header=None,delim_whitespace=True)\n",
        "data\n",
        "# 3196rows x 37 columns\n",
        "\n",
        "# b.Transform the DataFrame into a form that the apyori system can use.\n",
        "records = []\n",
        "for i in range(len(data)):\n",
        "    records.append([str(data.iloc[i, j]) for j in range(data.shape[1])])\n",
        "\n",
        "records = []\n",
        "for i in range(0, 3196):\n",
        "  records.append([str(data.values[i,j]) for j in range(0,37)])\n",
        "print(len(records))\n",
        "\n",
        "# c.generate rules,  with length 2 (no longer or shorter), support at least 0.01, confidence at least 2 and lift at least 3\n",
        "rules = apriori(records, min_support=0.01, min_confidence=2, min_lift=3, min_length=2, max_length=2)\n",
        "results = list(rules)\n",
        "print(len(results))\n",
        "\n",
        "# d.print out the rules in a readable form\n",
        "for item in results:\n",
        "  pair = item[0]\n",
        "  items = [x for x in pair]\n",
        "  print(\"Rule: \"+items[0]+ \" -> \" + items[1])\n",
        "  print(\"Support: \" + str(item[1]))\n",
        "  print(\"Confidence: \"+ str(item[2][0][2]))\n",
        "  print(\"Lift: \"+ str(item[2][0][3]))\n",
        "  print(\"======================================\")"
      ],
      "metadata": {
        "id": "rFX-cUZ3QqbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering _lab 4"
      ],
      "metadata": {
        "id": "TAHFPZjpmTlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Unsupervised learning: Clustering\n",
        "# lab 4 clustering k-means,gaussianmixture, EM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# no header and that the separators are spaces,没有头，分隔符是空格\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\",sep='\\s+', header = None)\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\",header=None,delim_whitespace=True)\n",
        "data.shape\n",
        "data.head()\n",
        "data[7].value_counts()\n",
        "\n",
        "# Extract columns:\n",
        "features = data.iloc[:,:7]\n",
        "labs = data.iloc[:,7]\n",
        "\n",
        "features = data.iloc[:,:-1]\n",
        "labs = data.iloc[:, -1]\n",
        "labs = labs-1\n",
        "# 因为labs是1,2,3,需要的是0,1,2\n",
        "\n",
        "# Apply k-means\n",
        "from sklearn.cluster import KMeans\n",
        "kms= KMeans(n_clusters=3)\n",
        "kms.fit(features)\n",
        "print(kms.labels_)\n",
        "# look at the predictions\n",
        "\n",
        "\n",
        "# evaluate the predictions\n",
        "from sklearn import metrics\n",
        "kms_score = metrics.rand_score(labs, kms.labels_)\n",
        "print(kms_score)\n",
        "\n",
        "\n",
        "# Apply EM\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components=3)\n",
        "gm.fit(features)\n",
        "prediction = gm.predict(features)\n",
        "print(prediction)\n",
        "\n",
        "\n",
        "gm_score = metrics.rand_score(labs, prediction)\n",
        "print(gm_score)\n",
        "\n",
        "if gm_score > kms_score:\n",
        "    print(\"EM/GMM is more accurate.\")\n",
        "else:\n",
        "    print(\"K-Means is more accurate.\")\n",
        "\n",
        "# Apply DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "features = np.array([[1,2],[2,2],[2,3],[8,7],[8,8],[25,80]])\n",
        "db = DBSCAN(eps=3, min_samples=2)\n",
        "dbscan = db.fit(features)\n",
        "print(dbscan.labels_)\n",
        "\n",
        "\n",
        "# 生成normal数据\n",
        "from numpy import hstack\n",
        "from numpy.random import normal\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# 由numpy.random.normal生成一个均值为20，标准差为5，含3000个数据点的正态分布数据集 X1\n",
        "# 由numpy.hstack 将X1和X2合并成一个包含10000个数据点的单一数组X。前3000个点来自X1，后7000个点来自X2\n",
        "# reshape 将X 转换为一个二维数组，形状为 (10000, 1)\n",
        "X1 = normal(loc=20, scale=5, size=3000)\n",
        "X2 = normal(loc=40, scale=5, size=7000)\n",
        "X = hstack((X1, X2))\n",
        "X = X.reshape((len(X),1))\n",
        "\n",
        "# 绘制X的直方图, bins=50,数据分成 50 个柱状图块\n",
        "pyplot.hist(X, bins=50)\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "gm = GaussianMixture(n_components=2)\n",
        "gm.fit(X)\n",
        "y = gm.predict(X)\n",
        "print(y[:100])\n",
        "print(y[-100:])\n",
        "\n",
        "Xnew, ynew = gm.sample(6)\n",
        "print(Xnew)\n",
        "print(ynew)"
      ],
      "metadata": {
        "id": "SHjxn8GmmTOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0000eb-5ba8-4b43-87d5-86a31a23a5c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-edec3008a64f>:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\",header=None,delim_whitespace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 2 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2]\n",
            "0.8743677375256322\n",
            "[2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 2 2 2 2 2 0 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 2 2 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "0.9115516062884484\n",
            "EM/GMM is more accurate.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARM"
      ],
      "metadata": {
        "id": "CNhcSM3R09DC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MntC2F7k0yLQ",
        "outputId": "c0dbf965-d38a-4326-f4d0-492589383621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apyori\n",
            "  Downloading apyori-1.1.2.tar.gz (8.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: apyori\n",
            "  Building wheel for apyori (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apyori: filename=apyori-1.1.2-py3-none-any.whl size=5954 sha256=9e2bc5a4903df105a8c799a9a9afc4983a3362d56b40659950fee7a1470e0a41\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/3d/a6/d317a6fb32be58a602b1e8c6b5d6f31f79322da554cad2a5ea\n",
            "Successfully built apyori\n",
            "Installing collected packages: apyori\n",
            "Successfully installed apyori-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install apyori"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ARM(Unsupervised learning: association rule mining)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "\n",
        "data = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/78a9bd799b5fbf35344beff50304169f789d264c/Market_Basket_Optimisation.csv?raw=true\", header=None)\n",
        "\n",
        "# 有7501行,20列, transform tha dataframe into a form that the apyori system can use.\n",
        "print(data)\n",
        "records = []\n",
        "for i in range(0, 7501):\n",
        "  records.append([str(data.values[i,j]) for j in range(0,20)])\n",
        "\n",
        "#support: 35/7500 = 0.0045, lift提升度：规则的有效性,length=2,关联规则至少包含2个商品\n",
        "rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\n",
        "results = list(rules)\n",
        "print(len(results))\n",
        "print(results[0])\n",
        "\n",
        "for item in results:\n",
        "  pair = item[0]\n",
        "  items = [x for x in pair]\n",
        "  print(\"Rule: \"+items[0]+ \" -> \" + items[1])\n",
        "  print(\"Support: \" + str(item[1]))\n",
        "  print(\"Confidence: \"+ str(item[2][0][2]))\n",
        "  print(\"Lift: \"+ str(item[2][0][3]))\n",
        "  print(\"======================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhshTki808P7",
        "outputId": "f2a59abb-406b-42df-a26e-44ddb23423e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exam 2024\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "\n",
        "data = pd.read_csv(\"http://fimi.uantwerpen.be/data/mushroom.dat\",header=None,delim_whitespace=True)\n",
        "data\n",
        "\n",
        "records = []\n",
        "for i in range(0, 8124):\n",
        "  records.append([str(data.values[i,j]) for j in range(0,23)])\n",
        "print(len(records))\n",
        "\n",
        "# length exactly =2\n",
        "rules = apriori(records, min_support=0.02, min_confidence=0.5, min_lift=40, min_length=2, max_length=2)\n",
        "results = list(rules)\n",
        "print(len(results))\n",
        "\n",
        "for item in results:\n",
        "  pair = item[0]\n",
        "  items = [x for x in pair]\n",
        "  print(\"Rule: \"+items[0]+ \" -> \" + items[1])\n",
        "  print(\"Support: \" + str(item[1]))\n",
        "  print(\"Confidence: \"+ str(item[2][0][2]))\n",
        "  print(\"Lift: \"+ str(item[2][0][3]))\n",
        "  print(\"======================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhFFNfpQFEcq",
        "outputId": "9a952ce9-1cc5-4b90-fdd1-3c9fa097c9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-4f0cc62c6285>:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  data = pd.read_csv(\"http://fimi.uantwerpen.be/data/mushroom.dat\",header=None,delim_whitespace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8124\n",
            "1\n",
            "Rule: 73 -> 83\n",
            "Support: 0.023633677991137372\n",
            "Confidence: 1.0\n",
            "Lift: 42.3125\n",
            "======================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction\n",
        "- feature selection: Variance Threshold, Chi2\n",
        "    - method approaches:\n",
        "      - filter\n",
        "      - wrapper\n",
        "      - embedded\n",
        "- feature extraction/learning:PCA"
      ],
      "metadata": {
        "id": "DWvF8TzR9suZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature selection:\n",
        "# filter method: use a proxy measure\n",
        "# Chi2 - categorical\n",
        "\n",
        "# VarianceThreshold 方差选择，方差大于等于 0.1 的特征会被保留\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=0.1)\n",
        "X_new = selector.fit_transform(X)\n",
        "\n",
        "# Chi2 卡方校验,适用于非负数据（比如计数数据或频率），常用于分类任务\n",
        "# k=2 表示选择前2个得分最高的特征,(衡量特征和目标之间的独立性：卡方值越大，说明特征和目标越相关)\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# PCA\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "X = np.array([[-1,1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "# 解释方差比例\n",
        "print(pca.explained_variance_ratio_)\n",
        "# 输出奇异值\n",
        "print(pca.singular_values_)"
      ],
      "metadata": {
        "id": "5on4bA2u9sbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural networks"
      ],
      "metadata": {
        "id": "n7J0kXJmXEt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data\")\n",
        "data\n",
        "# two attribute be predicted, motor_UPDRS, and total_UPDRS,choose 1 of these, and delete the other, and drop subject#(is just a row number)\n",
        "data.drop(labels=['subject#', 'motor_UPDRS'], axis=1, inplace=True)\n",
        "\n",
        "# split into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "datatrain, datatest = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# extract labels\n",
        "trainlabs = datatrain.loc[:,'total_UPDRS']\n",
        "traindata = datatrain.drop(labels=['total_UPDRS'], axis=1)\n",
        "testlabs = datatest.loc[:,'total_UPDRS']\n",
        "testdata = datatest.drop(labels=['total_UPDRS'], axis=1)\n",
        "\n",
        "# rescale based on the training data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sclaer = StandardScaler()\n",
        "traindatar = sclaer.fit_transform(traindata)\n",
        "testdatar = sclaer.transform(testdata)\n",
        "\n",
        "# try 3 regressors, A linear one\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(linreg, traindatar, trainlabs, scoring=\"neg_mean_squared_error\", cv=split)\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"LS RMSE =\", rmse.mean(), \" stddev =\", scores.std())\n",
        "# LS RMSE = 9.692187534169982  stddev = 2.304681679339098\n",
        "\n",
        "# try MLPRegressor\n",
        "# MLP with default parameters, mlpreg = MLPRegressor()\n",
        "# 修改参数：MLPRegressor(hidden_layer_sizes=(20,20,20)),表示有三个网络层,第一hiddenlayer有20个神经元,第2hidden layer有20个神经元,第3hidden layer有20个神经元,\n",
        "# MLPRegressor(hidden_layer_sizes=(50, 30), activation='relu', max_iter=1000, random_state=42), 两个层，分别50和30个神经元,激活函数relu,\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "mlpreg = MLPRegressor() # rmse = 7.9, stddev = 2.8\n",
        "mlpreg = MLPRegressor(hidden_layer_sizes=(20,20,20)) #rmse=6.9, stddev=5.01\n",
        "scores = cross_val_score(mlpreg, traindatar, trainlabs, scoring=\"neg_mean_squared_error\", cv=split)\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"MLP RMSE =\", rmse.mean(), \" stddev =\", scores.std())\n",
        "\n",
        "# apply the regressors to the test data\n",
        "# fit each regressor again, find their predictions, and compute the errors\n",
        "from sklearn.metrics import mean_squared_error\n",
        "linreg.fit(traindatar, trainlabs)\n",
        "y = linreg.predict(testdatar)\n",
        "mse = mean_squared_error(testlabs, y)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"LIN test error = \", rmse)\n",
        "# LIN test error =  9.741855670538472\n",
        "\n",
        "# mlp\n",
        "mlpreg.fit(traindatar, trainlabs)\n",
        "y = mlpreg.predict(testdatar)\n",
        "mse = mean_squared_error(testlabs, y)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"MLP test error = \", rmse)\n",
        "# MLP test error =  6.093603183111678"
      ],
      "metadata": {
        "id": "N9kAXSINXD6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP classification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\")\n",
        "\n",
        "# drop stab, and leaving stabf as the class label\n",
        "data = data.drop(\"stab\", axis=1)\n",
        "datavars = data.drop(\"stabf\", axis=1)\n",
        "datalabs = data[\"stabf\"].copy()\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "datavarsr = scaler.fit_transform(datavars)\n",
        "\n",
        "le = LabelEncoder()\n",
        "datalabse = le.fit_transform(datalabs)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(200), max_iter=1000)\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, datavarsr, datalabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"MLP accuracy =\",scores.mean(),\" stddev =\", scores.std())\n",
        "# MLP accuracy = 0.9647500000000001  stddev = 0.0035443617196894497"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQAfjTQNg0Xe",
        "outputId": "27723d07-2023-4bd8-bb0f-9aa838ad33a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP accuracy = 0.9647500000000001  stddev = 0.0035443617196894497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary classification with Keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv\")\n",
        "\n",
        "data = data.drop(\"stab\", axis=1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "datatrain, datatest = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# drop an attribute and separating labels\n",
        "traindata = datatrain.drop(\"stabf\", axis=1)\n",
        "trainlabs = datatrain[\"stabf\"].copy()\n",
        "testdata = datatest.drop(\"stabf\", axis=1)\n",
        "testlabs = datatest[\"stabf\"].copy()\n",
        "\n",
        "# rescaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "traindatar = scaler.fit_transform(traindata)\n",
        "testdatar = scaler.transform(testdata)\n",
        "\n",
        "# label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "trainlabse = le.fit_transform(trainlabs)\n",
        "testlabse = le.transform(testlabs)\n",
        "\n",
        "# use Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# for binary classification we need a sigmoid neuron to give a probability(0-1)\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(200, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# compile the model(编译模型)\n",
        "model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# model fit the dataset\n",
        "model.fit(traindatar, trainlabse, epochs=50)\n",
        "\n",
        "# apply the trained network to the test data\n",
        "model.evaluate(testdatar, testlabse)\n",
        "# accuracy: 0.94"
      ],
      "metadata": {
        "id": "1FcAb3cRl0nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiclass classification with Keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# reade data\n",
        "datatrain = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\", header=None)\n",
        "datatest = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\", header=None)\n",
        "\n",
        "# split off labels\n",
        "traindata = datatrain.drop(64, axis=1)\n",
        "trainlabs = datatrain[64].copy()\n",
        "testdata = datatest.drop(64, axis=1)\n",
        "testlabs = datatest[64].copy()\n",
        "\n",
        "# one-hot encoder\n",
        "# sparse =false, 默认会返回稀疏矩阵，但是false后，返回普通numpy数组(密集矩阵)\n",
        "# enc = OneHotEncoder(sparse=False) 旧版本\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "trainlabs = enc.fit_transform(trainlabs.to_numpy().reshape(-1,1))\n",
        "testlabs = enc.transform(testlabs.to_numpy().reshape(-1,1))\n",
        "\n",
        "# if the input is 2D image, we should first use Flatten layer, again with an input_shape, 28*28images\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "\n",
        "# use Keras, create the model\n",
        "# add a dense layer of ReLU neurons\n",
        "# as for binary classification, add a softmax layer of 10 neurons (输出10个类所以写10)\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "# loss function use categorical_crossentropy\n",
        "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# fit the network to the data\n",
        "model.fit(traindata, trainlabs, epochs=50)\n",
        "\n",
        "# evaluate the testing data accuracy: 0.9587\n",
        "model.evaluate(testdata, testlabs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nnB_K7cgx3_d",
        "outputId": "9d106875-43dc-4d98-cb8f-c8ee11035861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3580 - loss: 3.9527\n",
            "Epoch 2/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8850 - loss: 0.3783\n",
            "Epoch 3/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9415 - loss: 0.1940\n",
            "Epoch 4/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1249\n",
            "Epoch 5/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1018\n",
            "Epoch 6/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.0778\n",
            "Epoch 7/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0662\n",
            "Epoch 8/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0637\n",
            "Epoch 9/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0532\n",
            "Epoch 10/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9842 - loss: 0.0484\n",
            "Epoch 11/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0398\n",
            "Epoch 12/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0307\n",
            "Epoch 13/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0340\n",
            "Epoch 14/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9888 - loss: 0.0301\n",
            "Epoch 15/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0270\n",
            "Epoch 16/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0189\n",
            "Epoch 17/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0186\n",
            "Epoch 18/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9949 - loss: 0.0182\n",
            "Epoch 19/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0132\n",
            "Epoch 20/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9976 - loss: 0.0125\n",
            "Epoch 21/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0111\n",
            "Epoch 22/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9962 - loss: 0.0108\n",
            "Epoch 23/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0112\n",
            "Epoch 24/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0063\n",
            "Epoch 25/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0060\n",
            "Epoch 26/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0065\n",
            "Epoch 27/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0058\n",
            "Epoch 28/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0052\n",
            "Epoch 29/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0061\n",
            "Epoch 30/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0045\n",
            "Epoch 31/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0033\n",
            "Epoch 32/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0040\n",
            "Epoch 33/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0035\n",
            "Epoch 34/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0037\n",
            "Epoch 35/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0019\n",
            "Epoch 36/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0030\n",
            "Epoch 37/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0026\n",
            "Epoch 38/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0023\n",
            "Epoch 39/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0016\n",
            "Epoch 40/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0019\n",
            "Epoch 41/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0021\n",
            "Epoch 42/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0023\n",
            "Epoch 43/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0018\n",
            "Epoch 44/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0018\n",
            "Epoch 45/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0011\n",
            "Epoch 46/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 8.3854e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0011\n",
            "Epoch 48/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0011\n",
            "Epoch 49/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0012\n",
            "Epoch 50/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0011\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7add96f91f90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout for overfitting\n",
        "# Dropout is one of the most popular ways of preventing overfifitting\n",
        "# Dropout only has an effect during fitting: when a trained network is evaluated, it just passes data unchanged\n",
        "\n",
        "# 参数 0.1：表示每次训练时，随机丢弃 10% 的输入单元\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(Dropout(0.1, input_shape=(64,)))\n",
        "model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(traindata, trainlabs, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-NWb0XPA3uwX",
        "outputId": "1c6652ec-8cd8-482b-ca61-f53b1bfbe958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.3170 - loss: 3.8461\n",
            "Epoch 2/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7525 - loss: 0.7343\n",
            "Epoch 3/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8363 - loss: 0.5243\n",
            "Epoch 4/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8656 - loss: 0.4081\n",
            "Epoch 5/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8778 - loss: 0.3799\n",
            "Epoch 6/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.3216\n",
            "Epoch 7/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9082 - loss: 0.2810\n",
            "Epoch 8/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9102 - loss: 0.2701\n",
            "Epoch 9/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9241 - loss: 0.2265\n",
            "Epoch 10/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9224 - loss: 0.2418\n",
            "Epoch 11/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9303 - loss: 0.2128\n",
            "Epoch 12/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9314 - loss: 0.2090\n",
            "Epoch 13/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9398 - loss: 0.1899\n",
            "Epoch 14/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2231\n",
            "Epoch 15/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1901\n",
            "Epoch 16/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9409 - loss: 0.2176\n",
            "Epoch 17/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9458 - loss: 0.1808\n",
            "Epoch 18/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9395 - loss: 0.1756\n",
            "Epoch 19/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9434 - loss: 0.1729\n",
            "Epoch 20/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9494 - loss: 0.1554\n",
            "Epoch 21/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.1668\n",
            "Epoch 22/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9421 - loss: 0.1819\n",
            "Epoch 23/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9392 - loss: 0.1800\n",
            "Epoch 24/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9354 - loss: 0.1860\n",
            "Epoch 25/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1436\n",
            "Epoch 26/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9531 - loss: 0.1378\n",
            "Epoch 27/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9441 - loss: 0.1589\n",
            "Epoch 28/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.1497\n",
            "Epoch 29/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9448 - loss: 0.1647\n",
            "Epoch 30/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1472\n",
            "Epoch 31/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9474 - loss: 0.1529\n",
            "Epoch 32/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.1315\n",
            "Epoch 33/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9499 - loss: 0.1686\n",
            "Epoch 34/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9615 - loss: 0.1299\n",
            "Epoch 35/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9493 - loss: 0.1392\n",
            "Epoch 36/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9531 - loss: 0.1299\n",
            "Epoch 37/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.1475\n",
            "Epoch 38/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9588 - loss: 0.1223\n",
            "Epoch 39/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.1358\n",
            "Epoch 40/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1252\n",
            "Epoch 41/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1249\n",
            "Epoch 42/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1351\n",
            "Epoch 43/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9503 - loss: 0.1347\n",
            "Epoch 44/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1196\n",
            "Epoch 45/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9555 - loss: 0.1435\n",
            "Epoch 46/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1133\n",
            "Epoch 47/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1141\n",
            "Epoch 48/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1356\n",
            "Epoch 49/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9574 - loss: 0.1299\n",
            "Epoch 50/50\n",
            "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1204\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7add96fa5110>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression with Keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# read data\n",
        "housing = pd.read_csv(\"https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.csv\")\n",
        "\n",
        "# splite data\n",
        "trainset, testset = train_test_split(housing, test_size=0.2)\n",
        "\n",
        "# Drop a categorical variable, split train and test sets into data and labels\n",
        "traindata = trainset.drop(\"median_house_value\", axis=1)\n",
        "trainlabs = trainset[\"median_house_value\"].copy()\n",
        "trainnum = traindata.drop(\"ocean_proximity\", axis=1)\n",
        "testdata = testset.drop(\"median_house_value\", axis=1)\n",
        "testlabs = testset[\"median_house_value\"].copy()\n",
        "testnum = testdata.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "# Defin new attributes\n",
        "trainnum[\"rooms_per_household\"] = trainnum[\"total_rooms\"] /trainnum[\"households\"]\n",
        "trainnum[\"bedrooms_per_room\"] = trainnum[\"total_bedrooms\"] /trainnum[\"total_rooms\"]\n",
        "trainnum[\"population_per_household\"] = trainnum[\"population\"] /trainnum[\"households\"]\n",
        "testnum[\"rooms_per_household\"] = testnum[\"total_rooms\"] /testnum[\"households\"]\n",
        "testnum[\"bedrooms_per_room\"] = testnum[\"total_bedrooms\"] /testnum[\"total_rooms\"]\n",
        "testnum[\"population_per_household\"] = testnum[\"population\"] /testnum[\"households\"]\n",
        "\n",
        "# Impute\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "imputer.fit(trainnum)\n",
        "X = imputer.transform(trainnum)\n",
        "trainnum = pd.DataFrame(X, columns=trainnum.columns)\n",
        "Y = imputer.transform(testnum)\n",
        "testnum = pd.DataFrame(Y, columns=testnum.columns)\n",
        "\n",
        "# Rescale\n",
        "scaler = StandardScaler()\n",
        "trainnumr = scaler.fit_transform(trainnum)\n",
        "testnumr = scaler.transform(testnum)\n",
        "\n",
        "\n",
        "# for regression the final layer is a single neuron\n",
        "# apply keras,  used 3 hidden layers with 100 neurons each:\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\", input_shape=(11,)))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(keras.layers.Dense(1))\n",
        "model.compile(loss=\"mean_squared_error\")\n",
        "model.fit(trainnumr, trainlabs, epochs=50)\n",
        "\n",
        "# evaluate data\n",
        "model.evaluate(testnumr, testlabs)\n",
        "\n",
        "# =================== Keras allows shorthand notation for the sequential model\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(100, activation=\"relu\", input_shape=(11,)),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "hP0MK4Ud6UuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}