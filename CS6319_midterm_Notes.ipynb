{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i88xWaLhP4YM",
        "Udtr9yM2Pv_U",
        "xrq0TiJDUJTq",
        "x3C4E4YYw3Ty"
      ],
      "authorship_tag": "ABX9TyOHC0Zkrvuknsce16cTahhz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADinng/CS6319_MachineLearning/blob/main/CS6319_midterm_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "nHxDIwVuk7PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2024midterm_regression"
      ],
      "metadata": {
        "id": "i88xWaLhP4YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://github.com/andvise/DataAnalyticsDatasets/blob/main/Data.csv?raw=true\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# 2. check data\n",
        "data.head()\n",
        "data.describe()\n",
        "data.shape\n",
        "\n",
        "# 3. add column name\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "data[0].value_counts()\n",
        "\n",
        "# 4. check for miss value\n",
        "data.isnull().sum()\n",
        "\n",
        "# 5. hand miss value\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "datanum = data.iloc[:,1:]\n",
        "datacat = data.iloc[:,0]\n",
        "\n",
        "imputer.fit(datanum)\n",
        "X = imputer.transform(datanum)\n",
        "datanum= pd.DataFrame(X, columns=datanum.columns)\n",
        "data = pd.concat([datacat, datanum], axis=1)\n",
        "\n",
        "# 6. split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# 7. check features (if corr_matrix<0.1 then Remove features that do not correlate with the label)\n",
        "corr_matrix=trainset.drop(0, axis=1).corr()\n",
        "print(corr_matrix[8])\n",
        "\n",
        "# 8. create labs\n",
        "traindata = trainset.iloc[:,:-1]\n",
        "trainlabs = trainset.iloc[:,-1]\n",
        "testdata = testset.iloc[:,:-1]\n",
        "testlabs = testset.iloc[:,-1]\n",
        "\n",
        "# 9. create cat and num\n",
        "traincat = traindata[[0]]\n",
        "trainnum = traindata.drop(0, axis=1)\n",
        "\n",
        "testcat = testdata[[0]]\n",
        "testnum = testdata.drop(0, axis=1)\n",
        "\n",
        "# 10. use standscaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "trainnumr = scaler.fit_transform(trainnum)\n",
        "testnumr = scaler.transform(testnum)\n",
        "\n",
        "# 11.use one hot encoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "trainhot = cat_encoder.fit_transform(traincat).toarray()\n",
        "trainnew = np.concatenate([trainnumr, trainhot], axis=1)\n",
        "\n",
        "testhot = cat_encoder.transform(testcat).toarray()\n",
        "testnew = np.concatenate([testnumr, testhot], axis=1)\n",
        "\n",
        "# 12.use shuffle and randomforest\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "\n",
        "rfreg = RandomForestRegressor()\n",
        "scores = cross_val_score(rfreg, trainnew, trainlabs, scoring=\"neg_mean_squared_error\", cv=split)\n",
        "rmse=np.sqrt(-scores)\n",
        "print(\"XVALSHUF RF RMSE mean = \",rmse.mean(),\" stddev =\",rmse.std())\n",
        "\n",
        "# 12.1 use KNN (rmse越小越好)\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knnreg = KNeighborsRegressor()\n",
        "scores = cross_val_score(knnreg, trainnew, trainlabs, scoring=\"neg_mean_squared_error\", cv=split)\n",
        "rmse=np.sqrt(-scores)\n",
        "print(\"XVALSHUF KNN RMSE mean = \",rmse.mean(),\" stddev =\",rmse.std())\n",
        "\n",
        "\n",
        "# 13. use grid search to tune be hyperparameters of the winning regressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # 树的数量\n",
        "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
        "    'min_samples_split': [2, 5, 10],  # 分裂内部节点所需的最小样本数\n",
        "    'min_samples_leaf': [1, 2, 4],    # 叶节点所需的最小样本数\n",
        "    'max_features': ['log2', 'sqrt']   # 寻找最佳分割时考虑的特征数量\n",
        "}\n",
        "\n",
        "rfreg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# 创建网格搜索对象\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rfreg,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',  # 使用负均方误差作为评分标准\n",
        "    cv=5,  # 5折交叉验证\n",
        "    n_jobs=2,  # 使用所有可用的CPU核心\n",
        ")\n",
        "\n",
        "grid_search.fit(trainnew, trainlabs)\n",
        "\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "\n",
        "\n",
        "# 14. evaluate the best-performing regressor on the test data\n",
        "best_rfreg = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_rfreg.predict(testnew)\n",
        "test_rmse = np.sqrt(mean_squared_error(testlabs, y_pred))\n",
        "print(\"Test RMSE with best model: \", test_rmse)"
      ],
      "metadata": {
        "id": "FNcO4ohhP3iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lecturenote"
      ],
      "metadata": {
        "id": "Udtr9yM2Pv_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. import data read csv\n",
        "house = pd.read_csv(\"https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.csv\")\n",
        "\n",
        "# 2. examine the dataframe\n",
        "house.head()\n",
        "house.info()\n",
        "house[\"ocean_proximity\"].value_counts() # 检查类别分布\n",
        "house.describe()\n",
        "\n",
        "# 3. Split dataset into training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "trainset, testset = train_test_split(house, test_size=0.2)\n",
        "\n",
        "# 4. separate the labels (columns (axis 1) or rows (axis 0))\n",
        "traindata = trainset.drop(\"median_house_value\", axis=1)\n",
        "trainlabs = trainset[\"median_house_value\"].copy()\n",
        "\n",
        "# 5.Separate categorical and numerical attributes\n",
        "traincat = traindata[[\"ocean_proximity\"]]\n",
        "trainnum = traindata.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "# 6.Handling missing values(处理缺失值) mean均值，median中位数，most_frequent众数\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "imputer.fit(trainnum)\n",
        "X = imputer.transform(trainnum)\n",
        "trainnum= pd.DataFrame(X, columns=trainnum.columns)\n",
        "\n",
        "# 7.rescaler to the traindata(数据缩放)StandardScaler (将数据标准化为均值 0 和标准差 1)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "trainnumr = scaler.fit_transform(trainnum)\n",
        "\n",
        "# 8. fit regressor to the training data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(trainnumr, trainlabs)\n",
        "predictions=linreg.predict(trainnumr)\n",
        "\n",
        "# 9. use mse and rmse (rmse 越小，模型预测越准确 ,关注异常值,mae, 稳健易解释的指标)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(trainlabs, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Train numeries LIN error = \", rmse)\n",
        "\n",
        "# 10. create new features (combining numerical attributes)\n",
        "trainnum[\"rooms_per_household\"]=trainnum[\"total_rooms\"]/trainnum[\"households\"]\n",
        "trainnum[\"bedrooms_per_room\"]= trainnum[\"total_bedrooms\"]/trainnum[\"total_rooms\"]\n",
        "trainnum[\"population_per_household\"]=trainnum[\"population\"]/trainnum[\"households\"]\n",
        "\n",
        "imputer.fit(trainnum)\n",
        "X = imputer.transform(trainnum)\n",
        "trainnum = pd.DataFrame(X, columns=trainnum.columns)\n",
        "trainnumr = scaler.fit_transform(trainnum)\n",
        "\n",
        "\n",
        "# 10.1 Feature selection_spearman's corrleation(if < 0.2 can drop, >0.9 or ==1, can drop)\n",
        "corr_matrix=trainset.drop(\"ocean_proximity\", axis=1).corr()\n",
        "print(corr_matrix[\"median_house_value\"])\n",
        "\n",
        "# longitude -0.047998 latitude -0.143619 housing_median_age 0.111647 total_rooms 0.140739 total_bedrooms 0.055268 population -0.018309 households 0.071222 median_income 0.693173 median_house_value 1.000000 Name: median_house_value, dtype: float64 %%\n",
        "\n",
        "housing.plot(x=\"median_income\", y=\"median_house_value\", kind=\"scatter\", alpha=0.1)\n",
        "\n",
        "trainnum1 = trainnumr[:, [1, 2, 3, 7, 9, 10]] linreg.fit(trainnum1, trainlabs) predictions = linreg.predict(trainnum1)\n",
        "mse = mean_squared_error(trainlabs, predictions)\n",
        "rmse = np.sqrt(mse) print(\"Train selected numericas LIN error = \", rmse)\n",
        "# Train selected numericas LIN error = 77316.06904450871 %%\n",
        "\n",
        "\n",
        "# 11. use onehot encode\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "trainhot = cat_encoder.fit_transform(traincat).toarray()\n",
        "trainnew = np.concatenate([trainnumr, trainhot], axis=1)\n",
        "\n",
        "linreg.fit(trainnew, trainlabs)\n",
        "predictions = linreg.predict(trainnew)\n",
        "mse = mean_squared_error(trainlabs, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Train combined arrributes LIN error = \", rmse)\n",
        "# Train combined arrributes LIN error = 67765.74947521035\n",
        "\n",
        "\n",
        "# 12. other regression\n",
        "# DecisionTree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "treereg = DecisionTreeRegressor()\n",
        "treereg.fit(trainnew, trainlabs)\n",
        "predictions = treereg.predict(trainnew)\n",
        "mse = mean_squared_error(trainlabs, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Train combined attributes DT error = \", rmse)\n",
        "# Train combined attributes DT error = 0.0\n",
        "\n",
        "# 13. use testdata for evaluation, and the training data for learning\n",
        "testdata = testset.drop(\"median_house_value\", axis=1)\n",
        "testlabs = testset[\"median_house_value\"].copy()\n",
        "\n",
        "testnum = testdata.drop(\"ocean_proximity\", axis=1)\n",
        "testcat = testdata[[\"ocean_proximity\"]]\n",
        "\n",
        "testnum[\"rooms_per_household\"] = testnum[\"total_rooms\"]/testnum[\"households\"]\n",
        "testnum[\"bedrooms_per_room\"] = testnum[\"total_bedrooms\"]/testnum[\"total_rooms\"]\n",
        "testnum[\"population_per_household\"] = testnum[\"population\"] / testnum[\"households\"]\n",
        "\n",
        "X = imputer.transform(testnum)\n",
        "testnum = pd.DataFrame(X, columns=testnum.columns)\n",
        "testnumr = scaler.transform(testnum)\n",
        "\n",
        "testhot = cat_encoder.fit_transform(testcat).toarray()\n",
        "testnew = np.concatenate([testnumr, testhot], axis=1)\n",
        "\n",
        "# random forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfreg = RandomForestRegressor()\n",
        "rfreg.fit(trainnew, trainlabs)\n",
        "\n",
        "predictions = rfreg.predict(testnew)\n",
        "mse = mean_squared_error(testlabs, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Test combined attributes RF error = \", rmse)\n",
        "# Test combined attributes RF error = 51474.77108788628\n",
        "\n",
        "# 14. cross-validation\n",
        "# 14.1 k-fold cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(linreg, trainnew, trainlabs, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "rmse = np.sqrt(-scores)\n",
        "print(\"XVAL LIN RMSE mean =\",rmse.mean(),\" stddev =\",rmse.std())\n",
        "#XVAL LIN RMSE mean = 67929.13193437677 stddev = 2602.8838783869105\n",
        "\n",
        "# 14.2 Shuffle split cross-validataion\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(linreg, trainnew, trainlabs, scoring=\"neg_mean_squared_error\", cv=split)\n",
        "rmse=np.sqrt(-scores)\n",
        "print(\"XVALSHUF LIN RMSE mean = \",rmse.mean(),\" stddev =\",rmse.std())\n"
      ],
      "metadata": {
        "id": "Q4wcOdbzlDiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "Lb_aF2zMok32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.2023midterm classification"
      ],
      "metadata": {
        "id": "xrq0TiJDUJTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. import data\n",
        "#(You should take into account the fact that values are separated by spaces instead of commas, and thatthe file has no header.)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\"\n",
        "\n",
        "data = pd.read_csv(url, sep='\\s+', header=None)\n",
        "\n",
        "# 2. check data\n",
        "data.head()\n",
        "data.describe()\n",
        "\n",
        "# 3.split data\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "traindata = trainset.iloc[:,1:9]\n",
        "trainlabs = trainset.iloc[:,9]\n",
        "testdata = testset.iloc[:,1:9]\n",
        "testlabs = testset.iloc[:,9]\n",
        "\n",
        "# 4. check miss value\n",
        "missing_values = data.isnull().sum()\n",
        "missing_values\n",
        "\n",
        "# 5. minmax\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "traindatar = scaler.fit_transform(traindata)\n",
        "testdatar = scaler.transform(testdata)\n",
        "\n",
        "# 6. LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "trainlabse = le.fit_transform(trainlabs)\n",
        "testlabse = le.transform(testlabs)\n",
        "\n",
        "# 7. LinearSVC\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, traindatar, trainlabse, scoring=\"f1_macro\", cv=split)\n",
        "print(\"LS f1 mean =\", scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "# 8.random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "scores = cross_val_score(classifier, traindatar, trainlabse, scoring=\"f1_macro\", cv = split)\n",
        "print(\"RF f1 mean = \", scores.mean(),\" stddev = \", scores.std())\n",
        "\n",
        "# 9.KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier()\n",
        "scores = cross_val_score(classifier, traindatar, trainlabse, scoring=\"f1_macro\", cv = split)\n",
        "print(\"KN f1 mean = \", scores.mean(),\" stddev = \", scores.std())\n",
        "\n",
        "# 10.use the best to fit test value\n",
        "best_clf = KNeighborsClassifier()\n",
        "best_clf.fit(traindatar, trainlabse)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = best_clf.predict(testdatar)\n",
        "test_accuracy = accuracy_score(testlabse, y_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9CycQDhUIlC",
        "outputId": "579f4bfd-791d-4a25-f802-327f22ad963f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LS f1 mean = 0.46000339526240674  stddev = 0.051681193222509504\n",
            "RF f1 mean =  0.47375143443529816  stddev =  0.053952010212195804\n",
            "KN f1 mean =  0.5465495971996273  stddev =  0.03785665844100078\n",
            "Test Accuracy: 0.5926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. binary classification"
      ],
      "metadata": {
        "id": "x3C4E4YYw3Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# binary classification: electrical grid stability\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "# 1. import data\n",
        "griddata = pd.read_csv(\"https://archive.ics.uci.edu/ml/machinelearning-databases/00471/Data_for_UCI_named.csv\")\n",
        "griddata = griddata.drop(\"stab\", axis=1)\n",
        "gridvars = griddata.drop(\"stabf\", axis=1)\n",
        "gridlabs = griddata[\"stabf\"].copy()\n",
        "\n",
        "# 2.check for missing values\n",
        "gridvars.info()\n",
        "gridvars.describe()\n",
        "\n",
        "\n",
        "# 3.use normalisation or standardisation to make all variables between 0 and 1.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "gridvarsr = scaler.fit_transform(gridvars)\n",
        "\n",
        "# 4. label encode\n",
        "# sklearn classifiers expect class labels to be in the form 0,1,2,...\n",
        "# we need to transform the stable/unstable values.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "gridlabse = le.fit_transform(gridlabs)\n",
        "\n",
        "# 5. fit classifier\n",
        "# 5.1 linear support vector classifier, instead use accuracy\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"LS acc mean =\",scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "\n",
        "# 5.2 decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"DT acc mean =\",scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "# 5.3 random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"RT acc mean =\",scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "\n",
        "# 5.4 Support vector classifier\n",
        "from sklearn.svm import SVC\n",
        "classifier = SVC()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"SV acc mean =\",scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "# 5.5 K-Nearest-Neighbours\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring=\"accuracy\", cv=split)\n",
        "print(\"KN acc mean =\",scores.mean(),\" stddev =\",scores.std())"
      ],
      "metadata": {
        "id": "cQq3w0wZutMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple classes"
      ],
      "metadata": {
        "id": "lwJoCmMiyinW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "imbalanced data"
      ],
      "metadata": {
        "id": "mPYBHQTZzFqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "mnisttra = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\",header=None)\n",
        "mnisttes = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes\",header=None)\n",
        "\n",
        "# split label\n",
        "mnisttrad = mnisttra.drop(64, axis=1)\n",
        "mnisttesd = mnisttes.drop(64, axis=1)\n",
        "mnisttral = mnisttra[64].copy()\n",
        "mnisttesl = mnisttes[64].copy()\n",
        "ytra = (mnisttral == 5)\n",
        "ytes = (mnisttesl == 5)\n",
        "\n",
        "# label encoder\n",
        "le = LabelEncoder()\n",
        "ytrae = le.fit_transform(ytra)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
        "classifier = LinearSVC()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "score = cross_val_score(classifier, mnisttrad, ytrae, scoring=\"accuracy\", cv=split)\n",
        "print(\"LS accuracy mena=\", score.mean())\n",
        "\n",
        "# using accuracy isn't appropriate for imbalance data\n",
        "\n",
        "# decisiontree and f1\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "scores = cross_val_score(classifier, mnisttrad, ytrae, scoring=\"f1\", cv = split)\n",
        "print(\"DT f1 mean = \", score.mean(),\" stddev = \", scores.std())\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "scores = cross_val_score(classifier, mnisttrad, ytrae, scoring=\"f1\", cv = split)\n",
        "print(\"RF f1 mean = \", score.mean(),\" stddev = \", scores.std())\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier()\n",
        "scores = cross_val_score(classifier, mnisttrad, ytrae, scoring=\"f1\", cv = split)\n",
        "print(\"KN f1 mean = \", score.mean(),\" stddev = \", scores.std())\n",
        "\n",
        "\n",
        "# confusion matrix\n",
        "mnisttrale = le.fit_transform(mnisttral)\n",
        "classifier = DecisionTreeClassifier()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, mnisttrad, mnisttrale, scoring=\"accuracy\", cv=split)\n",
        "print(\"DT accuracy mean = \",scores.mean(), \" stddev = \", scores.std())\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "# generate a confusion matrix for multiclass problem\n",
        "ypred = cross_val_predict(classifier, mnisttrad, mnisttrale, cv=3)\n",
        "conf = confusion_matrix(ypred, mnisttrale)\n",
        "print(conf)\n",
        "\n",
        "import matplotlib as plt\n",
        "plt.pyplot.matshow(conf, cmap=plt.cm.gray)\n",
        "rowsums = conf.sum(axis=1, keepdims=True)\n",
        "normconf = conf/rowsums\n",
        "np.fill_diagonal(normconf, 0)\n",
        "\n",
        "plt.pyplot.matshow(normconf, cmap=plt.cm.gray)\n"
      ],
      "metadata": {
        "id": "ThjRGSV6ypOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. import data\n",
        "firewall = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00542/log2.csv\")\n",
        "\n",
        "#categories very imblanced --> use F1 scoring\n",
        "firewall[\"Action\"].value_counts()\n",
        "# 2. split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "firetrain, firetest = train_test_split(firewall, test_size=0.2)\n",
        "\n",
        "# 3. choose labs and drop data\n",
        "# iloc[row index, colum index] 左闭又开，traindata 只取5-10列\n",
        "traindata = firetrain.iloc[:,5:11]\n",
        "trainlabs = firetrain.iloc[:,4]\n",
        "testdata = firetest.iloc[:,5:11]\n",
        "testlabs = firetest.iloc[:,4]\n",
        "\n",
        "# 4. data rescaled (MinMax将值缩放到 [0, 1])\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "traindatar = scaler.fit_transform(traindata)\n",
        "testdatar = scaler.transform(testdata)\n",
        "\n",
        "#5. encode the text labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "trainlabse = le.fit_transform(trainlabs)\n",
        "testlabse = le.transform(testlabs)\n",
        "\n",
        "#6. fit data LinearSVC\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "split = ShuffleSplit(n_splits=10, test_size=0.2)\n",
        "scores = cross_val_score(classifier, traindatar, trainlabse, scoring=\"f1_macro\", cv=split)\n",
        "print(\"LS f1 mean =\", scores.mean(),\" stddev =\",scores.std())\n",
        "\n",
        "#7. Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "params =[{'n_neighbors': [3, 5, 8, 13],\n",
        "          'weights': ['uniform','distance'],\n",
        "          'p':[1,2]}]\n",
        "classifier = KNeighborsClassifier()\n",
        "search = GridSearchCV(classifier, params, cv=5, scoring =\"f1_macro\")\n",
        "search.fit(traindatar, trainlabse)\n",
        "\n",
        "print(\"best params: \", search.best_params_)\n",
        "print(\"best score: \", search.best_score_)\n",
        "\n",
        "#best params:  {'n_neighbors': 8, 'p': 1, 'weights': 'distance'}\n",
        "#best score:  0.7476954686129262\n",
        "\n",
        "# 7.2 优化\n",
        "params =[{'n_neighbors': [6, 8, 10, 15, 18]}]\n",
        "classifier = KNeighborsClassifier(weights='distance',p=1)\n",
        "search = GridSearchCV(classifier, params, cv=5, scoring =\"f1_macro\")\n",
        "search.fit(traindatar, trainlabse)\n",
        "\n",
        "print(\"best params: \", search.best_params_)\n",
        "print(\"best score: \", search.best_score_)\n",
        "#best params:  {'n_neighbors': 18}\n",
        "#best score:  0.7487962725321669\n",
        "\n",
        "# 8. evaluate on test data\n",
        "from sklearn.metrics import f1_score\n",
        "classifier = KNeighborsClassifier(n_neighbors=10, p=1, weights='distance')\n",
        "classifier.fit(traindatar, trainlabse)\n",
        "y1 = classifier.predict(testdatar)\n",
        "print(\" Test f1 = \", f1_score(testlabse, y1, average='macro'))"
      ],
      "metadata": {
        "id": "-PUzVPfWontz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}